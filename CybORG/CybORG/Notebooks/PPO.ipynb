{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from shutil import make_archive\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import ray\n",
    "from ray import tune\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EPS = 100\n",
    "agent_name = 'Blue'\n",
    "\n",
    "def wrap(env):\n",
    "    return ChallengeWrapper(env=env, agent_name='Blue')\n",
    "\n",
    "def evaluate(steps, trainer):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    obs = []\n",
    "    #print(f'using CybORG v{cyborg_version}, {scenario}\\n')\n",
    "    for num_steps in steps:\n",
    "        for red_agent in [B_lineAgent]:\n",
    "            cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "            wrapped_cyborg = wrap(cyborg)\n",
    "\n",
    "            observation = wrapped_cyborg.reset()\n",
    "            obs.append(observation)\n",
    "            # observation = cyborg.reset().observation\n",
    "\n",
    "            action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "            # action_space = cyborg.get_action_space(agent_name)\n",
    "            total_reward = []\n",
    "            actions = []\n",
    "            for i in range(MAX_EPS):\n",
    "                r = []\n",
    "                a = []\n",
    "                # cyborg.env.env.tracker.render()\n",
    "                for j in range(num_steps):\n",
    "                    action = trainer.compute_single_action(observation)\n",
    "                    action_vec = np.zeros(145)\n",
    "                    action_vec[int(action)] = 1\n",
    "                    #action = agent.get_action(observation, action_space)\n",
    "                    observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "                    obs.append(observation)\n",
    "                    actions.append(action_vec)\n",
    "                    # result = cyborg.step(agent_name, action)\n",
    "                    r.append(rew)\n",
    "                    # r.append(result.reward)\n",
    "                    a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "                total_reward.append(sum(r))\n",
    "                # actions.append(a)\n",
    "                # observation = cyborg.reset().observation\n",
    "                observation = wrapped_cyborg.reset()\n",
    "            print(f'Average reward for red agent {red_agent.__name__} and steps {num_steps} is: {mean(total_reward):.1f} with a standard deviation of {stdev(total_reward):.1f}')\n",
    "            return mean(total_reward), np.mean(np.array(obs), axis=0),  np.mean(np.array(actions), axis=0)\n",
    "\n",
    "def env_creator(env_config):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = ChallengeWrapper(env=cyborg, agent_name='Blue')\n",
    "    return env\n",
    "\n",
    "register_env(\"cyborg\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(config):\n",
    "\n",
    "    iterations = 1000 #config.pop(\"train-iterations\")\n",
    "    trainer = ppo.PPO(config=config, env=\"cyborg\")\n",
    "    checkpoint = None\n",
    "    train_results = {}\n",
    "    \n",
    "    allrewards = []\n",
    "    reward = []\n",
    "    novel_obs = []\n",
    "    novel_actions = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "        train_results = trainer.train()\n",
    "        if i % 100 == 0 or i == iterations - 1:\n",
    "            checkpoint = trainer.save(tune.get_trial_dir())\n",
    "            r, o, a = evaluate([100], trainer)\n",
    "            reward.append(r)\n",
    "            novel_obs.append(o)\n",
    "            novel_actions.append(a)\n",
    "        tune.report(**train_results)\n",
    "    trainer.stop()\n",
    "    allrewards.append(reward)\n",
    "    np.save('ppo_reward.npy', np.array(reward))\n",
    "    np.save('ppo_obs.npy', np.stack(novel_obs))\n",
    "    np.save('ppo_actions.npy', np.stack(novel_actions))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    ray.init()\n",
    "    config = ppo.DEFAULT_CONFIG.copy()\n",
    "    config['framework'] = \"tf\"\n",
    "    config['env'] = 'cyborg'\n",
    "    config['num_gpus'] = 1\n",
    "    config[\"num_workers\"] = 3\n",
    "    config['horizon'] = 1000\n",
    "    config['train_batch_size'] = 100\n",
    "    config['rollout_fragment_length'] = 100\n",
    "    config['model'] = {\n",
    "        \"fcnet_hiddens\": [512, 512],\n",
    "        \"fcnet_activation\": \"relu\"\n",
    "    }\n",
    "    config['batch_mode'] = \"truncate_episodes\"\n",
    "    config['lambda'] = 0.95\n",
    "    config['kl_coeff'] = 0.5\n",
    "    config['clip_rewards'] = True\n",
    "    config['clip_param'] = 0.1\n",
    "    config['vf_clip_param'] = 10.0\n",
    "    config['entropy_coeff'] = 0.01\n",
    "    # config['sgd_minibatch_size'] = 32\n",
    "    # config['num_sgd_iter'] = 10\n",
    "    config['vf_share_layers'] = True\n",
    "    # config[\"exploration_config\"] = {\n",
    "    #     \"type\": \"Curiosity\",  # <- Use the Curiosity module for exploring.\n",
    "    #     \"eta\": 1.0,  # Weight for intrinsic rewards before being added to extrinsic ones.\n",
    "    #     \"lr\": 0.001,  # Learning rate of the curiosity (ICM) module.\n",
    "    #     \"feature_dim\": 288,  # Dimensionality of the generated feature vectors.\n",
    "    #     # Setup of the feature net (used to encode observations into feature (latent) vectors).\n",
    "    #     \"feature_net_config\": {\n",
    "    #         \"fcnet_hiddens\": [],\n",
    "    #         \"fcnet_activation\": \"relu\",\n",
    "    #     },\n",
    "    #     \"inverse_net_hiddens\": [256],  # Hidden layers of the \"inverse\" model.\n",
    "    #     \"inverse_net_activation\": \"relu\",  # Activation of the \"inverse\" model.\n",
    "    #     \"forward_net_hiddens\": [256],  # Hidden layers of the \"forward\" model.\n",
    "    #     \"forward_net_activation\": \"relu\",  # Activation of the \"forward\" model.\n",
    "    #     \"beta\": 0.2,  # Weight for the \"forward\" loss (beta) over the \"inverse\" loss (1.0 - beta).\n",
    "    #     # Specify, which exploration sub-type to use (usually, the algo's \"default\"\n",
    "    #     # exploration, e.g. EpsilonGreedy for DQN, StochasticSampling for PG/SAC).\n",
    "    #     \"sub_exploration\": {\n",
    "    #         \"type\": \"StochasticSampling\",\n",
    "    #     }\n",
    "    # }\n",
    "\n",
    "    tune.run(\n",
    "        experiment,\n",
    "        config=config,\n",
    "        resources_per_trial=ppo.PPO.default_resource_request(config),\n",
    "        # resources_per_trial={'gpu': 1}\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0676da54bda53ba833e8fdd25024551ce9d082e3a1866964d054fa24018d0f8e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.11 ('cyborg2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
