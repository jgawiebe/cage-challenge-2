{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 12:24:46,121\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.execution.buffers` has been deprecated. Use `ray.rllib.utils.replay_buffers` instead. This will raise an error in the future!\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from shutil import make_archive\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import ray\n",
    "from ray import tune\n",
    "import ray.rllib.algorithms.impala as impala\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(env):\n",
    "    return ChallengeWrapper(env=env, agent_name='Blue')\n",
    "\n",
    "def env_creator(env_config):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = ChallengeWrapper(env=cyborg, agent_name='Blue')\n",
    "    return env\n",
    "\n",
    "register_env(\"cyborg\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=4292)\u001b[0m 2022-07-26 12:24:57,639\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.execution.buffers` has been deprecated. Use `ray.rllib.utils.replay_buffers` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=13248)\u001b[0m 2022-07-26 12:24:57,682\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.execution.buffers` has been deprecated. Use `ray.rllib.utils.replay_buffers` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=5116)\u001b[0m 2022-07-26 12:24:57,667\tWARNING deprecation.py:47 -- DeprecationWarning: `ray.rllib.execution.buffers` has been deprecated. Use `ray.rllib.utils.replay_buffers` instead. This will raise an error in the future!\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=4292)\u001b[0m 2022-07-26 12:24:58,017\tWARNING env.py:141 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-07-26 12:25:01,729\tINFO trainable.py:157 -- Trainable.setup took 15.408 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-07-26 12:25:01,815\tINFO trainable.py:589 -- Restored on 127.0.0.1 from checkpoint: C:/Users/Rafael/ray_results/experiment_2022-07-24_21-42-03/experiment_cyborg_fbc03_00000_0_2022-07-24_21-42-03/checkpoint_010000/checkpoint-10000\n",
      "2022-07-26 12:25:01,816\tINFO trainable.py:598 -- Current state after restoring: {'_iteration': 10000, '_timesteps_total': None, '_time_total': 103045.50446414948, '_episodes_total': 29382}\n"
     ]
    }
   ],
   "source": [
    "# Configuration used to train the model (not sure if needed)\n",
    "config = impala.DEFAULT_CONFIG.copy()\n",
    "config['framework'] = \"tf\"\n",
    "config['env'] = 'cyborg'\n",
    "config['num_gpus'] = 1\n",
    "config[\"num_workers\"] = 3\n",
    "config['horizon'] = 1024\n",
    "\n",
    "# Load the model from path\n",
    "trained_model_path = \"C:/Users/Rafael/ray_results/experiment_2022-07-24_21-42-03/experiment_cyborg_fbc03_00000_0_2022-07-24_21-42-03/checkpoint_010000/checkpoint-10000\"\n",
    "agent = impala.Impala(config=config, env=\"cyborg\")\n",
    "agent.restore(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION FOR Scenario2\n",
      "Average reward for red agent B_lineAgent and steps 30 is: -9.377999999999997 with a standard deviation of 4.049007360583655\n",
      "Average reward for red agent RedMeanderAgent and steps 30 is: -11.536000000000001 with a standard deviation of 2.5274860748549854\n",
      "Average reward for red agent SleepAgent and steps 30 is: 0.0 with a standard deviation of 0.0\n",
      "Average reward for red agent B_lineAgent and steps 50 is: -17.871 with a standard deviation of 3.956743253196653\n",
      "Average reward for red agent RedMeanderAgent and steps 50 is: -32.03199999999998 with a standard deviation of 7.790422506403796\n",
      "Average reward for red agent SleepAgent and steps 50 is: 0.0 with a standard deviation of 0.0\n",
      "Average reward for red agent B_lineAgent and steps 100 is: -36.89400000000006 with a standard deviation of 7.3765780027261405\n",
      "Average reward for red agent RedMeanderAgent and steps 100 is: -98.85799999999999 with a standard deviation of 31.14338031386023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-14:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Rafael\\Anaconda3\\envs\\cyborg2\\lib\\threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\Rafael\\Anaconda3\\envs\\cyborg2\\lib\\site-packages\\ray\\rllib\\execution\\learner_thread.py\", line 74, in run\n",
      "    self.step()\n",
      "  File \"c:\\Users\\Rafael\\Anaconda3\\envs\\cyborg2\\lib\\site-packages\\ray\\rllib\\execution\\multi_gpu_learner_thread.py\", line 143, in step\n",
      "    buffer_idx, released = self.ready_tower_stacks_buffer.get()\n",
      "  File \"c:\\Users\\Rafael\\Anaconda3\\envs\\cyborg2\\lib\\site-packages\\ray\\rllib\\execution\\minibatch_buffer.py\", line 48, in get\n",
      "    self.buffers[self.idx] = self.inqueue.get(timeout=self.timeout)\n",
      "  File \"c:\\Users\\Rafael\\Anaconda3\\envs\\cyborg2\\lib\\queue.py\", line 178, in get\n",
      "    raise Empty\n",
      "_queue.Empty\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average reward for red agent SleepAgent and steps 100 is: 0.0 with a standard deviation of 0.0\n"
     ]
    }
   ],
   "source": [
    "MAX_EPS = 100\n",
    "agent_name = 'Blue'\n",
    "scenarios = [\"Scenario2\"]\n",
    "steps = [30, 50, 100]\n",
    "red_agents = [B_lineAgent, RedMeanderAgent, SleepAgent]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    path = (str(inspect.getfile(CybORG))[:-10] + '/Shared/Scenarios/{}.yaml').format(scenario)\n",
    "    print(\"EVALUATION FOR\", scenario)\n",
    "    for num_steps in steps:\n",
    "        for red_agent in red_agents:\n",
    "            cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "            wrapped_cyborg = wrap(cyborg)\n",
    "            observation = wrapped_cyborg.reset()\n",
    "            # observation = cyborg.reset().observation\n",
    "            action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "            # action_space = cyborg.get_action_space(agent_name)\n",
    "            total_reward = []\n",
    "            actions = []\n",
    "            for i in range(MAX_EPS):\n",
    "                r = []\n",
    "                a = []\n",
    "                # cyborg.env.env.tracker.render()\n",
    "                for j in range(num_steps):\n",
    "                    # action = agent.get_action(observation, action_space)\n",
    "                    # action, _states = agent.predict(observation)\n",
    "                    action = agent.compute_single_action(observation) # RLlib \n",
    "                    # action, state_out, _ = agent.compute_single_action(observation, state) # RLlib with Attention \n",
    "                    observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "                    # result = cyborg.step(agent_name, action)\n",
    "                    r.append(rew)\n",
    "                    # r.append(result.reward)\n",
    "                    a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "                total_reward.append(sum(r))\n",
    "                actions.append(a)\n",
    "                # observation = cyborg.reset().observation\n",
    "                observation = wrapped_cyborg.reset()\n",
    "            print(f'Average reward for red agent {red_agent.__name__} and steps {num_steps} is: {mean(total_reward)} with a standard deviation of {stdev(total_reward)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cyborg2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0676da54bda53ba833e8fdd25024551ce9d082e3a1866964d054fa24018d0f8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
