{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from shutil import make_archive\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import ray\n",
    "from ray import tune\n",
    "import ray.rllib.algorithms.impala as impala\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(env):\n",
    "    return ChallengeWrapper(env=env, agent_name='Blue')\n",
    "\n",
    "def env_creator(env_config):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario2.yaml'\n",
    "    agents = {\"Red\": B_lineAgent, \"Green\": GreenAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = ChallengeWrapper(env=cyborg, agent_name='Blue')\n",
    "    return env\n",
    "\n",
    "register_env(\"cyborg\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 19:27:05,660\tWARNING ppo.py:350 -- `train_batch_size` (1024) cannot be achieved with your other settings (num_workers=3 num_envs_per_worker=1 rollout_fragment_length=100)! Auto-adjusting `rollout_fragment_length` to 342.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=1844)\u001b[0m 2022-07-18 19:27:14,766\tWARNING env.py:141 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-07-18 19:27:18,795\tINFO trainable.py:157 -- Trainable.setup took 13.136 seconds. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\n",
      "2022-07-18 19:27:19,004\tINFO trainable.py:589 -- Restored on 127.0.0.1 from checkpoint: C:/Users/Rafael/ray_results/experiment_2022-07-14_14-38-05-scenario2/experiment_cyborg_19644_00000_0_2022-07-14_14-38-05/checkpoint_010000/checkpoint-10000\n",
      "2022-07-18 19:27:19,005\tINFO trainable.py:598 -- Current state after restoring: {'_iteration': 10000, '_timesteps_total': None, '_time_total': 49130.859721660614, '_episodes_total': 10017}\n"
     ]
    }
   ],
   "source": [
    "# Configuration used to train the model (not sure if needed)\n",
    "config = impala.DEFAULT_CONFIG.copy()\n",
    "config['framework'] = \"tf\"\n",
    "config['env'] = 'cyborg'\n",
    "config['num_gpus'] = 1\n",
    "config[\"num_workers\"] = 3\n",
    "config['horizon'] = 1024\n",
    "\n",
    "# Load the model from path\n",
    "trained_model_path = \"C:/Users/Rafael/ray_results/experiment_2022-07-21_21-31-29/experiment_cyborg_0288e_00000_0_2022-07-21_21-31-29/checkpoint_005000/checkpoint-5000\"\n",
    "agent = impala.Impala(config=config, env=\"cyborg\")\n",
    "agent.restore(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION FOR Scenario2\n",
      "Average reward for red agent B_lineAgent and steps 30 is: -14.554999999999998 with a standard deviation of 15.04015752523351\n",
      "Average reward for red agent RedMeanderAgent and steps 30 is: -16.079 with a standard deviation of 5.062763548446121\n",
      "Average reward for red agent SleepAgent and steps 30 is: 0.0 with a standard deviation of 0.0\n",
      "Average reward for red agent B_lineAgent and steps 50 is: -23.675000000000004 with a standard deviation of 9.210362795692841\n",
      "Average reward for red agent RedMeanderAgent and steps 50 is: -62.583000000000006 with a standard deviation of 39.83942023988934\n",
      "Average reward for red agent SleepAgent and steps 50 is: 0.0 with a standard deviation of 0.0\n",
      "Average reward for red agent B_lineAgent and steps 100 is: -46.58300000000001 with a standard deviation of 17.292843472186274\n",
      "Average reward for red agent RedMeanderAgent and steps 100 is: -150.039 with a standard deviation of 63.682225443005265\n",
      "Average reward for red agent SleepAgent and steps 100 is: 0.0 with a standard deviation of 0.0\n"
     ]
    }
   ],
   "source": [
    "MAX_EPS = 100\n",
    "agent_name = 'Blue'\n",
    "scenarios = [\"Scenario2\"]\n",
    "steps = [30, 50, 100]\n",
    "red_agents = [B_lineAgent, RedMeanderAgent, SleepAgent]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    path = (str(inspect.getfile(CybORG))[:-10] + '/Shared/Scenarios/{}.yaml').format(scenario)\n",
    "    print(\"EVALUATION FOR\", scenario)\n",
    "    for num_steps in steps:\n",
    "        for red_agent in red_agents:\n",
    "            cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "            wrapped_cyborg = wrap(cyborg)\n",
    "            observation = wrapped_cyborg.reset()\n",
    "            # observation = cyborg.reset().observation\n",
    "            action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "            # action_space = cyborg.get_action_space(agent_name)\n",
    "            total_reward = []\n",
    "            actions = []\n",
    "            for i in range(MAX_EPS):\n",
    "                r = []\n",
    "                a = []\n",
    "                # cyborg.env.env.tracker.render()\n",
    "                for j in range(num_steps):\n",
    "                    # action = agent.get_action(observation, action_space)\n",
    "                    # action, _states = agent.predict(observation)\n",
    "                    action = agent.compute_single_action(observation) # RLlib \n",
    "                    # action, state_out, _ = agent.compute_single_action(observation, state) # RLlib with Attention \n",
    "                    observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "                    # result = cyborg.step(agent_name, action)\n",
    "                    r.append(rew)\n",
    "                    # r.append(result.reward)\n",
    "                    a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "                total_reward.append(sum(r))\n",
    "                actions.append(a)\n",
    "                # observation = cyborg.reset().observation\n",
    "                observation = wrapped_cyborg.reset()\n",
    "            print(f'Average reward for red agent {red_agent.__name__} and steps {num_steps} is: {mean(total_reward)} with a standard deviation of {stdev(total_reward)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nothing done.\n"
     ]
    }
   ],
   "source": [
    "%reset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cyborg2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0676da54bda53ba833e8fdd25024551ce9d082e3a1866964d054fa24018d0f8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
