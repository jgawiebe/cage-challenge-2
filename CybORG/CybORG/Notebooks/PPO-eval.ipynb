{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from shutil import make_archive\n",
    "from statistics import mean, stdev\n",
    "from CybORG import CybORG\n",
    "from CybORG.Agents import B_lineAgent, SleepAgent, GreenAgent\n",
    "from CybORG.Agents.SimpleAgents.Meander import RedMeanderAgent\n",
    "from CybORG.Agents.Wrappers import ChallengeWrapper\n",
    "import ray\n",
    "from ray import tune\n",
    "import ray.rllib.algorithms.ppo as ppo\n",
    "from ray.tune.registry import register_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap(env):\n",
    "    return ChallengeWrapper(env=env, agent_name='Blue')\n",
    "\n",
    "def env_creator(env_config):\n",
    "    path = str(inspect.getfile(CybORG))\n",
    "    path = path[:-10] + '/Shared/Scenarios/Scenario1b.yaml'\n",
    "    agents = {\"Red\": B_lineAgent}\n",
    "    cyborg = CybORG(scenario_file=path, environment='sim', agents=agents)\n",
    "    env = ChallengeWrapper(env=cyborg, agent_name='Blue')\n",
    "    return env\n",
    "\n",
    "register_env(\"cyborg\", env_creator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RolloutWorker pid=14848)\u001b[0m 2022-07-14 10:26:58,323\tWARNING env.py:141 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2022-07-14 10:27:00,972\tINFO trainable.py:589 -- Restored on 127.0.0.1 from checkpoint: C:/Users/Rafael/ray_results/experiment_2022-07-05_11-56-56/experiment_cyborg_18c39_00000_0_2022-07-05_11-56-56/checkpoint_005000/checkpoint-5000\n",
      "2022-07-14 10:27:00,973\tINFO trainable.py:598 -- Current state after restoring: {'_iteration': 5000, '_timesteps_total': None, '_time_total': 115647.05923509598, '_episodes_total': 25000}\n"
     ]
    }
   ],
   "source": [
    "# Configuration used to train the model (not sure if needed)\n",
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "# config['framework'] = \"tf\"\n",
    "# config['env'] = 'cyborg'\n",
    "# config['num_gpus'] = 1\n",
    "# config[\"num_workers\"] = 3\n",
    "# config['horizon'] = 1000\n",
    "# config['train_batch_size'] = 512\n",
    "# config['sgd_minibatch_size'] = 32\n",
    "# config['rollout_fragment_length'] = 100\n",
    "# # config['model'] = {\n",
    "# #     \"fcnet_hiddens\": [512, 512],\n",
    "# #     \"fcnet_activation\": \"relu\"\n",
    "# # }\n",
    "# config['batch_mode'] = \"truncate_episodes\"\n",
    "# config['lambda'] = 0.95\n",
    "# config['kl_coeff'] = 0.5\n",
    "# config['clip_rewards'] = True\n",
    "# config['clip_param'] = 0.1\n",
    "# config['vf_clip_param'] = 10.0\n",
    "# config['entropy_coeff'] = 0.01\n",
    "# config['vf_share_layers'] = True\n",
    "\n",
    "# Load the model from path\n",
    "trained_model_path = \"C:/Users/Rafael/ray_results/experiment_2022-07-05_11-56-56/experiment_cyborg_18c39_00000_0_2022-07-05_11-56-56/checkpoint_005000/checkpoint-5000\"\n",
    "agent = ppo.PPO(config=config, env=\"cyborg\")\n",
    "agent.restore(trained_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EVALUATION FOR Scenario1b\n",
      "Average reward for red agent B_lineAgent and steps 30 is: -9.336 with a standard deviation of 3.8067595382185857\n",
      "Average reward for red agent RedMeanderAgent and steps 30 is: -11.942 with a standard deviation of 2.712454626534811\n",
      "Average reward for red agent SleepAgent and steps 30 is: -0.01 with a standard deviation of 0.1\n",
      "Average reward for red agent B_lineAgent and steps 50 is: -14.744 with a standard deviation of 5.654377405018508\n",
      "Average reward for red agent RedMeanderAgent and steps 50 is: -24.82 with a standard deviation of 6.035166304521554\n",
      "Average reward for red agent SleepAgent and steps 50 is: 0.0 with a standard deviation of 0.0\n",
      "EVALUATION FOR Scenario2\n",
      "Average reward for red agent B_lineAgent and steps 30 is: -159.87899999999996 with a standard deviation of 82.86070859757913\n",
      "Average reward for red agent RedMeanderAgent and steps 30 is: -30.994999999999997 with a standard deviation of 16.444552535134793\n",
      "Average reward for red agent SleepAgent and steps 30 is: 0.0 with a standard deviation of 0.0\n",
      "Average reward for red agent B_lineAgent and steps 50 is: -409.8570000000002 with a standard deviation of 127.94593575318288\n",
      "Average reward for red agent RedMeanderAgent and steps 50 is: -242.953 with a standard deviation of 81.8574906699923\n",
      "Average reward for red agent SleepAgent and steps 50 is: 0.0 with a standard deviation of 0.0\n"
     ]
    }
   ],
   "source": [
    "MAX_EPS = 100\n",
    "agent_name = 'Blue'\n",
    "scenarios = [\"Scenario1b\", \"Scenario2\"]\n",
    "steps = [30, 50]\n",
    "red_agents = [B_lineAgent, RedMeanderAgent, SleepAgent]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    path = (str(inspect.getfile(CybORG))[:-10] + '/Shared/Scenarios/{}.yaml').format(scenario)\n",
    "    print(\"EVALUATION FOR\", scenario)\n",
    "    for num_steps in steps:\n",
    "        for red_agent in red_agents:\n",
    "            cyborg = CybORG(path, 'sim', agents={'Red': red_agent})\n",
    "            wrapped_cyborg = wrap(cyborg)\n",
    "            observation = wrapped_cyborg.reset()\n",
    "            # observation = cyborg.reset().observation\n",
    "            action_space = wrapped_cyborg.get_action_space(agent_name)\n",
    "            # action_space = cyborg.get_action_space(agent_name)\n",
    "            total_reward = []\n",
    "            actions = []\n",
    "            for i in range(MAX_EPS):\n",
    "                r = []\n",
    "                a = []\n",
    "                # cyborg.env.env.tracker.render()\n",
    "                for j in range(num_steps):\n",
    "                    # action = agent.get_action(observation, action_space)\n",
    "                    # action, _states = agent.predict(observation)\n",
    "                    action = agent.compute_single_action(observation) # RLlib \n",
    "                    # action, state_out, _ = agent.compute_single_action(observation, state) # RLlib with Attention \n",
    "                    observation, rew, done, info = wrapped_cyborg.step(action)\n",
    "                    # result = cyborg.step(agent_name, action)\n",
    "                    r.append(rew)\n",
    "                    # r.append(result.reward)\n",
    "                    a.append((str(cyborg.get_last_action('Blue')), str(cyborg.get_last_action('Red'))))\n",
    "                total_reward.append(sum(r))\n",
    "                actions.append(a)\n",
    "                # observation = cyborg.reset().observation\n",
    "                observation = wrapped_cyborg.reset()\n",
    "            print(f'Average reward for red agent {red_agent.__name__} and steps {num_steps} is: {mean(total_reward)} with a standard deviation of {stdev(total_reward)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('cyborg2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0676da54bda53ba833e8fdd25024551ce9d082e3a1866964d054fa24018d0f8e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
